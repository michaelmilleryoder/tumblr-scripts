{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McNemar's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712670 712670\n"
     ]
    }
   ],
   "source": [
    "# get true labels\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "data_dirpath = '/usr2/mamille2/tumblr/data/sample1k'\n",
    "\n",
    "feature_tables_dir = os.path.join(data_dirpath, 'feature_tables')\n",
    "filenames = ['reblog_features.csv', 'nonreblog_features.csv', 'ranking_labels.csv']\n",
    "joined_filenames = [os.path.join(feature_tables_dir, filename) for filename in filenames]\n",
    "# csv_readers = [csv.DictReader(codecs.open(filename, 'rU', 'utf-16')) for filename in joined_filenames]\n",
    "csv_readers = [csv.DictReader(x.replace('\\0', '') for x in open(filename, 'r')) for filename in joined_filenames]\n",
    "\n",
    "instances = []\n",
    "instance_labels = []\n",
    "for row in zip(*csv_readers):\n",
    "    reblog_features = row[0]\n",
    "    nonreblog_features = row[1]\n",
    "    label = int(row[2]['ranking_label'])\n",
    "    instance = (reblog_features, nonreblog_features) # reblog always first, nonreblog always second\n",
    "    instances.append(instance)\n",
    "    instance_labels.append(label)\n",
    "    \n",
    "print(len(instances), len(instance_labels))\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for (reblog_candidate, nonreblog_candidate), label in zip(instances, instance_labels):\n",
    "    X.append([])\n",
    "    y.append(label)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.1, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_categories = ['age', 'ethnicity/nationality', 'fandoms', 'gender',\n",
    "                       'interests', 'location', 'personality type', 'pronouns', 'relationship status', 'roleplay',\n",
    "                       'sexual orientation', 'zodiac']\n",
    "len(identity_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "statistic=89.030, p-value=0.000000\n",
      "Different proportions of errors (reject H0)\n",
      "\n",
      "ethnicity/nationality\n",
      "statistic=16.680, p-value=0.000044\n",
      "Different proportions of errors (reject H0)\n",
      "\n",
      "fandoms\n",
      "statistic=1.388, p-value=0.238725\n",
      "Same proportions of errors (fail to reject H0)\n",
      "\n",
      "gender\n",
      "statistic=62.811, p-value=0.000000\n",
      "Different proportions of errors (reject H0)\n",
      "\n",
      "interests\n",
      "statistic=98.539, p-value=0.000000\n",
      "Different proportions of errors (reject H0)\n",
      "\n",
      "location\n",
      "statistic=22.172, p-value=0.000002\n",
      "Different proportions of errors (reject H0)\n",
      "\n",
      "personality type\n",
      "statistic=0.000, p-value=1.000000\n",
      "Same proportions of errors (fail to reject H0)\n",
      "\n",
      "pronouns\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "data_dirpath = '/usr2/mamille2/tumblr/data/sample1k'\n",
    "\n",
    "for category in identity_categories + ['all']:\n",
    "    print(category)\n",
    "    baseline_pred = np.loadtxt(os.path.join(data_dirpath, 'output', 'predictions', f'lr_baseline+exp1_{category.replace(\"/\", \"_\").replace(\" \", \"_\")}_test_preds.txt'))\n",
    "    experiment_pred = np.loadtxt(os.path.join(data_dirpath, 'output', 'predictions', f'lr_baseline+exp2_{category.replace(\"/\", \"_\").replace(\" \", \"_\")}_test_preds.txt'))\n",
    "#     print(baseline_pred.shape)\n",
    "#     print(experiment_pred.shape)\n",
    "\n",
    "    a = 0\n",
    "    b = 0 # Baseline correct, experiment incorrect\n",
    "    c = 0 # Baseline incorrect, experiment correct\n",
    "    d = 0\n",
    "    for b_pred, ex_pred, true in zip(baseline_pred, experiment_pred, y_test):\n",
    "        if b_pred == true and ex_pred == true:\n",
    "            a += 1\n",
    "        elif b_pred == true and ex_pred != true:\n",
    "            b += 1\n",
    "        elif b_pred != true and ex_pred == true:\n",
    "            c += 1\n",
    "        else:\n",
    "            d += 1\n",
    "\n",
    "    table = [[a, b],\n",
    "             [c, d]]\n",
    "#     print(table)\n",
    "\n",
    "    # Example of calculating the mcnemar test\n",
    "    from statsmodels.stats.contingency_tables import mcnemar\n",
    "    # calculate mcnemar test\n",
    "    result = mcnemar(table, exact=False, correction=False)\n",
    "    # summarize the finding\n",
    "    print('statistic=%.3f, p-value=%.6f' % (result.statistic, result.pvalue))\n",
    "    # interpret the p-value\n",
    "    alpha = 0.05\n",
    "    if result.pvalue > alpha:\n",
    "        print('Same proportions of errors (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different proportions of errors (reject H0)')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

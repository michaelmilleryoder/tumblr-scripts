{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "data_dirpath = '/usr2/mamille2/tumblr/data/sample1k'\n",
    "\n",
    "feature_tables_dir = os.path.join(data_dirpath, 'feature_tables')\n",
    "filenames = ['reblog_features.csv', 'nonreblog_features.csv', 'ranking_labels.csv']\n",
    "joined_filenames = [os.path.join(feature_tables_dir, filename) for filename in filenames]\n",
    "# csv_readers = [csv.DictReader(codecs.open(filename, 'rU', 'utf-16')) for filename in joined_filenames]\n",
    "csv_readers = [csv.DictReader(x.replace('\\0', '') for x in open(filename, 'r')) for filename in joined_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712670 712670\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "instance_labels = []\n",
    "for row in zip(*csv_readers):\n",
    "    reblog_features = row[0]\n",
    "    nonreblog_features = row[1]\n",
    "    label = int(row[2]['ranking_label'])\n",
    "    instance = (reblog_features, nonreblog_features) # reblog always first, nonreblog always second\n",
    "    instances.append(instance)\n",
    "    instance_labels.append(label)\n",
    "    \n",
    "print(len(instances), len(instance_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tag vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14318\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def _str2list(in_str):\n",
    "    return [el[1:-1] for el in in_str[1:-1].split(', ')]\n",
    "\n",
    "def update_tag_counts(tag_counts, counted_ids, candidate): # for hashtags\n",
    "#     candidate_tags = [tag.lower() for tag in eval(candidate['post_tags'])] # uses tokens provided in feature tables\n",
    "    candidate_tags = [tag.lower() for tag in _str2list(candidate['post_tags'])] # uses tokens provided in feature tables\n",
    "    followee_id = candidate['tumblog_id_followee']    \n",
    "    for tag in candidate_tags:\n",
    "        if not followee_id in counted_ids[tag]: # only counts the tag if user hasn't already used the tag\n",
    "            tag_counts[tag] += 1\n",
    "            counted_ids[tag].add(followee_id)\n",
    "        \n",
    "counted_ids = defaultdict(lambda: set()) # for each tag, a set of followees who used those tags\n",
    "tag_counts = defaultdict(int) # count of unique followees who used each tag\n",
    "for reblog_candidate, nonreblog_candidate in instances:\n",
    "    update_tag_counts(tag_counts, counted_ids, reblog_candidate)\n",
    "    update_tag_counts(tag_counts, counted_ids, nonreblog_candidate)\n",
    "\n",
    "tag_counts_filtered = {k:v for k,v in tag_counts.items() if v > 1} # at least 2 users used the tag\n",
    "tag_vocab = tag_counts_filtered.keys()\n",
    "print(len(tag_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_categories = ['age', 'ethnicity/nationality', 'fandoms', 'gender',\n",
    "                       'interests', 'location', 'personality type', 'pronouns', 'relationship status', 'roleplay',\n",
    "                       'sexual orientation', 'weight', 'zodiac']\n",
    "len(identity_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count category label instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_label_counts = defaultdict(lambda: defaultdict(int)) # {category: {value: count_of_unique_users}}\n",
    "# counted_ids = set()\n",
    "for category in identity_categories:\n",
    "    counted_ids = set() # for each category, ids already considered\n",
    "    for reblog_candidate, nonreblog_candidate in instances:\n",
    "        category_followee = category + '_terms_followee'\n",
    "        followee_id = reblog_candidate['tumblog_id_followee']\n",
    "        if not followee_id in counted_ids: # only counts labels from first instance seen of a followee, since constant\n",
    "            category_value = [x.lower() for x in eval(reblog_candidate[category_followee])]\n",
    "            for value in category_value:\n",
    "                category_label_counts[category][value] += 1\n",
    "            counted_ids.add(followee_id)\n",
    "            \n",
    "        followee_id = nonreblog_candidate['tumblog_id_followee']\n",
    "        if not followee_id in counted_ids:\n",
    "            category_value = [x.lower() for x in eval(nonreblog_candidate[category_followee])]\n",
    "            for value in category_value:\n",
    "                category_label_counts[category][value] += 1\n",
    "            counted_ids.add(followee_id)\n",
    "        \n",
    "        category_follower = category + '_terms_follower'\n",
    "        follower_id = reblog_candidate['tumblog_id_follower']\n",
    "        if not follower_id in counted_ids:\n",
    "            category_value = [x.lower() for x in eval(reblog_candidate[category_follower])]\n",
    "            for value in category_value:\n",
    "                category_label_counts[category][value] += 1\n",
    "            counted_ids.add(follower_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create category label vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age 65\n",
      "{'12', '22', '34', '56', '47', 'forty', '13', 'twelve', '31', '29', '44', '50', '39', 'age', '55', '38', '17', '45', 'seventeen', '43', '11', 'fifty', '57', 'nineteen', '58', 'xxiv', '28', '32', '23', '25', 'y/o', 'twenty', '46', 'xix', '42', '33', '53', '37', '36', '26', '16', '48', 'fifteen', '14', '41', 'eighteen', '54', 'xxix', '10', '40', '20', '51', '24', 'fourteen', '27', '21', '49', '30', '35', 'sixteen', '15', '19', '18', '59', '52'}\n",
      "-----------------\n",
      "\n",
      "ethnicity/nationality 81\n",
      "{'chilean', 'finnish', 'russian', 'mexicana', 'coeur', 'singaporean', 'thais', 'dutch', 'americans', 'austrian', 'english', 'spanish', 'armenian', 'thai', 'asian', 'jamaican', 'black', 'swedish', 'chilena', 'portuguese', 'moroccan', 'tigre', 'swiss', 'romanian', 'indian', 'canadian', 'german', 'puerto', 'indonesian', 'malay', 'greek', 'turks', 'filipino', 'serbian', 'chinese', 'cuban', 'belgian', 'southern', 'vietnamese', 'chileno', 'dakota', 'danish', 'colombian', 'american', 'malaysian', 'south african', 'brazilian', 'mexican', 'norwegian', 'mexicano', 'hungarian', 'italian', 'latina', 'czech', 'white', 'african', 'irish', 'european', 'venezuelan', 'ottawa', 'latino', 'swede', 'hispanic', 'british', 'filipina', 'scandinavian', 'papel', 'pakistani', 'scottish', 'dominican', 'french', 'saudi', 'japanese', 'australian', 'korean', 'haitian', 'albanian', 'turkish', 'polish', 'omaha', 'lithuanian'}\n",
      "-----------------\n",
      "\n",
      "fandoms 54\n",
      "{'fanatic', 'star wars', 'ereri', 'kpop', 'otaku', 'riverdale', 'pokemon', 'ships', 'i ship', 'reylo', 'fandom', 'voltron', 'wwe', 'multiship', 'army', 'got7', 'yuri', 'hamilton', 'verse', 'rey', 'supercorp', 'k-pop', 'undertale', 'mcu', 'exo', 'hogwarts', 'shipper', 'sims', '5sos', 'phan', 'marvel', 'tjlc', 'bts', 'canon', 'bnha', 'shaladin', 'multiverse', 'stan', 'swiftie', 'homestuck', 'yaoi', 'sherlock', 'twd', 'fan', 'potterhead', 'harry potter', 'multi-fandom', 'multi-ship', 'overwatch', 'universe', 'comic', 'disney', 'hp', 'series'}\n",
      "-----------------\n",
      "\n",
      "gender 33\n",
      "{'queen', 'nonbinary', 'female', 'male', 'woman', 'trans', 'nb', 'androgynous', 'genderfluid', 'princess', 'bigender', 'wife', 'son', 'husband', 'gurl', 'man', 'lady', 'agender', 'lgbt', 'non-binary', 'girl', 'dad', 'mtf', 'brother', 'mum', 'boy', 'daughter', 'guy', 'ftm', 'sister', 'mommy', 'mom', 'cis'}\n",
      "-----------------\n",
      "\n",
      "interests 71\n",
      "{'meme', 'nature', 'music', 'writing', 'law', 'draw', 'comics', 'soccer', 'clarinet', 'bands', 'fitness', 'travel', 'gifs', 'games', 'weed', 'horror', 'manga', 'tv', 'pizza', 'drawing', 'family', 'running', 'books', 'photography', 'history', 'art', 'poetry', 'anime', 'animals', 'lifestyle', 'tattoos', 'makeup', 'tea', 'landscapes', 'film', 'desserts', 'design', 'book', 'interests', 'reading', 'pies', 'theatre', 'hair', 'movies', 'animal', 'hunting', 'food', 'gaming', 'dog', 'coffee', 'arts', 'write', 'memes', 'photos', 'dogs', 'aesthetic', 'fashion', 'hockey', 'aesthetics', 'meche', 'science', 'cat', 'cosplay', 'things', 'piano', 'metal', 'psychology', 'robot', 'cats', 'stuff', 'nutrition'}\n",
      "-----------------\n",
      "\n",
      "location 146\n",
      "{'taiwan', 'oklahoma', 'alabama', 'indiana', 'nz', 'jordan', 'oregon', 'va', 'fl', 'sweden', 'maine', 'israel', 'venezuela', 'bulgaria', 'switzerland', 'philadelphia', 'poland', 'china', 'hungary', 'iowa', 'croatia', 'arizona', 'ireland', 'alaska', 'toronto', 'south', 'midwest', 'madagascar', 'slovenia', 'tampa', 'atl', 'georgia', 'kansas', 'london', 'arkansas', 'colombia', 'az', 'nc', 'pennsylvania', 'ca', 'wisconsin', 'india', 'missouri', 'indonesia', 'ga', 'hong kong', 'atlanta', 'japan', 'north', 'nj', 'new hampshire', 'chad', 'france', 'norway', 'italy', 'louisiana', 'washington', 'hawaii', 'montana', 'virginia', 'athens', 'wa', 'romania', 'jamaica', 'nyc', 'mexico', 'netherlands', 'greece', 'chicago', 'minnesota', 'turkey', 'new zealand', 'east', 'albania', 'socal', 'united states', 'istanbul', 'south africa', 'ohio', 'malaysia', 'costa rica', 'usa', 'maryland', 'dc', 'illinois', 'philippines', 'mx', 'finland', 'u.s.', 'kentucky', 'al', 'massachusetts', 'ny', 'mali', 'spain', 'méxico', 'berlin', 'ct', 'u.k', 'pa', 'i̇stanbul', 'delaware', 'thailand', 'north carolina', 'singapore', 'chile', 'denmark', 'texas', 'belgium', 'west', 'slovakia', 'tx', 'cleveland', 'vietnam', 'cali', 'florida', 'utah', 'san', 'iceland', 'czech republic', 'california', 'aus', 'pnw', 'south carolina', 'serbia', 'new jersey', 'australia', 'brazil', 'rhode island', 'new york', 'colorado', 'portugal', 'egypt', 'umass', 'germany', 'ecuador', 'guinea', 'michigan', 'u.s', 'tennessee', 'philly', 'argentina', 'canada', 'mississippi', 'austria', 'uk'}\n",
      "-----------------\n",
      "\n",
      "personality type 29\n",
      "{'esfj', 'antp', '4w5', 'gryffindor', 'infp', 'enfj', 'ambivert', 'enfp', 'neutral', 'hufflepuff', 'isfj', 'entp', 'estp', 'lawful', '5w6', 'esfp', 'chaotic', '5w4', 'slytherin', 'istj', 'intj', 'ravenclaw', 'entj', 'isfp', 'intp', 'introvert', 'istp', 'infj', 'extrovert'}\n",
      "-----------------\n",
      "\n",
      "pronouns 9\n",
      "{'he', 'she', 'they', 'him', 'her', 'itits', 'them', 'xe', 'pronouns'}\n",
      "-----------------\n",
      "\n",
      "relationship status 9\n",
      "{'spouse', 'taken', 'in a relationship', 'wife', 'engaged', 'couple', 'husband', 'married', 'single'}\n",
      "-----------------\n",
      "\n",
      "roleplay 7\n",
      "{'semi-selective', 'oc', 'muse', 'm!a', 'selective', 'roleplay', 'rp'}\n",
      "-----------------\n",
      "\n",
      "sexual orientation 18\n",
      "{'demisexual', 'gay', 'bi', 'aro/ace', 'wlw', 'ace', 'lgbt', 'straight', 'asexual', 'cishet', 'lesbian', 'pansexual', 'aro-ace', 'mlm', 'pan', 'bisexual', 'homo', 'queer'}\n",
      "-----------------\n",
      "\n",
      "weight 21\n",
      "{'anorexic', 'cw', 'gw1', 'weight', 'bulimia', 'hw', 'lbs', 'lw', 'thin', 'lb', 'gw2', 'eating disorder', 'ana', 'kg', 'gw', 'gw3', 'fat', 'pounds', 'eating disorders', 'anorexia', 'sw'}\n",
      "-----------------\n",
      "\n",
      "zodiac 7\n",
      "{'cancer', 'aries', 'gemini', 'leo', 'taurus', 'libra', 'virgo'}\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_vocabs = defaultdict(lambda: set())\n",
    "for identity_category in category_label_counts:\n",
    "    category_labels_filtered_vocab = set([k for k,v in category_label_counts[identity_category].items() if v > 1]) # min 2 users using label\n",
    "    category_vocabs[identity_category] = category_labels_filtered_vocab\n",
    "    print(identity_category, len(category_vocabs[identity_category]))\n",
    "    print(category_vocabs[identity_category])\n",
    "    print('-----------------')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    sorted_category_labels = sorted(category_labels_filtered[category].items(), key=lambda x: x[1], reverse=True)    \n",
    "    print(category)\n",
    "    print('-----------------')\n",
    "    for i in range(1, 21):\n",
    "        if i == len(sorted_category_labels):\n",
    "            break\n",
    "        print(sorted_category_labels[i][0], sorted_category_labels[i][1])\n",
    "    print('-----------------')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_post_baseline(reblog_candidate, nonreblog_candidate, label):\n",
    "    features = defaultdict(float) # {feat: count} for each instance\n",
    "    # Comparison space features\n",
    "    def _extract_features_post_baseline_candidate(candidate, incr):\n",
    "        candidate_tags = [tag.lower() for tag in eval(candidate['post_tags'])]\n",
    "        for tag in candidate_tags:\n",
    "            if tag.lower() in tag_vocab:\n",
    "                feat_tag = ('tag=%s' % tag.lower())\n",
    "                features[feat_tag] += incr\n",
    "\n",
    "        post_type = candidate['post_type']\n",
    "        feat_tag = ('post_type=%s' % post_type)\n",
    "        features[feat_tag] += incr\n",
    "        \n",
    "        try:\n",
    "            post_note_count = float(candidate['post_note_count'])\n",
    "        except ValueError as e:\n",
    "            post_note_count = 0.0\n",
    "            \n",
    "        features['post_note_count'] += incr * post_note_count\n",
    "        \n",
    "    # if randomly-generated label is 1, second candidate is reblog, so flip: -1 is whatever candidate should consider first\n",
    "    if label == 1: \n",
    "        _extract_features_post_baseline_candidate(nonreblog_candidate, incr=-1)\n",
    "        _extract_features_post_baseline_candidate(reblog_candidate, incr=1)\n",
    "    else:\n",
    "        _extract_features_post_baseline_candidate(reblog_candidate, incr=-1)\n",
    "        _extract_features_post_baseline_candidate(nonreblog_candidate, incr=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Identity framing, presence of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_experiment_1(reblog_candidate, nonreblog_candidate, label, categories):\n",
    "    # Baseline features\n",
    "#     features = defaultdict(float)\n",
    "    features = extract_features_post_baseline(reblog_candidate, nonreblog_candidate, label)\n",
    "    \n",
    "    # Follower features\n",
    "#     for identity_category in identity_categories:\n",
    "#         identity_category_follower = eval(reblog_candidate[identity_category + '_terms_follower'])\n",
    "#         follower_presence = len(identity_category_follower) > 0\n",
    "#         if follower_presence:\n",
    "#             feat_tag = ('follower_cat=%s' % identity_category)\n",
    "#             features[feat_tag] += 1\n",
    "            \n",
    "    # Follower-followee comparison space features\n",
    "    def _extract_features_experiment_1_candidate(candidate, incr):\n",
    "        \n",
    "        num_matches = 0\n",
    "        num_mismatched_follower_presents = 0\n",
    "        num_mismatched_followee_presents = 0\n",
    "        \n",
    "#         for identity_category in identity_categories:\n",
    "        for identity_category in categories:\n",
    "            identity_category_follower = eval(reblog_candidate[identity_category + '_terms_follower'])\n",
    "            follower_presence = len(identity_category_follower) > 0\n",
    "            identity_category_followee = eval(candidate[identity_category + '_terms_followee'])\n",
    "            followee_presence = len(identity_category_followee) > 0\n",
    "#             if followee_presence:\n",
    "#                 feat_tag = ('followee_cat=%s' % identity_category)\n",
    "#                 features[feat_tag] += incr\n",
    "\n",
    "            # Alignment features\n",
    "#             if ((follower_presence and followee_presence) or\n",
    "#                 (not follower_presence and not followee_presence)):\n",
    "            # AND\n",
    "            if (follower_presence and followee_presence): # AND\n",
    "                feat_tag = ('aligned_cat=%s' % identity_category)\n",
    "                features[feat_tag] += incr\n",
    "                num_matches += 1\n",
    "                \n",
    "            # XOR\n",
    "            if (follower_presence and not followee_presence): # XOR\n",
    "                feat_tag = ('mismatched_follower_presents_cat=%s' % identity_category)\n",
    "                features[feat_tag] += incr\n",
    "                feat_tag = ('xor_cat=%s' % identity_category)\n",
    "                features[feat_tag] += incr\n",
    "                num_mismatched_follower_presents += 1\n",
    "            elif (not follower_presence and followee_presence): # XOR\n",
    "                feat_tag = ('mismatched_followee_presents_cat=%s' % identity_category)\n",
    "                features[feat_tag] += incr\n",
    "                feat_tag = ('xor_cat=%s' % identity_category)\n",
    "                features[feat_tag] += incr\n",
    "                num_mismatched_followee_presents += 1\n",
    "                \n",
    "        # Number of matches\n",
    "        features['num_matches'] += num_matches * incr\n",
    "        features['num_mismatched_follower_presents'] += num_mismatched_follower_presents * incr\n",
    "        features['num_mismatched_followee_presents'] += num_mismatched_followee_presents * incr\n",
    "            \n",
    "    if label == 1:\n",
    "        _extract_features_experiment_1_candidate(nonreblog_candidate, incr=-1)\n",
    "        _extract_features_experiment_1_candidate(reblog_candidate, incr=1)\n",
    "    else:\n",
    "        _extract_features_experiment_1_candidate(reblog_candidate, incr=-1)\n",
    "        _extract_features_experiment_1_candidate(nonreblog_candidate, incr=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_experiment_2(reblog_candidate, nonreblog_candidate, label):\n",
    "    # Baseline features\n",
    "#     features = defaultdict(float)\n",
    "#     features = extract_features_post_baseline(reblog_candidate, nonreblog_candidate, label)\n",
    "    features = extract_features_experiment_1(reblog_candidate, nonreblog_candidate, label)\n",
    "\n",
    "    \n",
    "    # Follower features\n",
    "    for identity_category in identity_categories:\n",
    "        identity_category_follower = [x.lower() for x in eval(reblog_candidate[identity_category + '_terms_follower'])]\n",
    "        for identity_label in identity_category_follower:\n",
    "            if identity_label in category_vocabs[identity_category]:\n",
    "                feat_tag = ('cat=%s,follower_lab=%s' % (identity_category, identity_label))\n",
    "                features[feat_tag] += 1\n",
    "            \n",
    "    # Comparison space features\n",
    "    def _extract_features_experiment_2_candidate(candidate, incr):\n",
    "        for identity_category in identity_categories:\n",
    "            identity_category_follower = [x.lower() for x in eval(reblog_candidate[identity_category + '_terms_follower'])]\n",
    "            identity_category_followee = [x.lower() for x in eval(reblog_candidate[identity_category + '_terms_followee'])]\n",
    "            for identity_label_followee in identity_category_followee:\n",
    "                if identity_label_followee in category_vocabs[identity_category]:\n",
    "                    feat_tag = ('cat=%s,followee_lab=%s' % (identity_category, identity_label_followee))\n",
    "                    features[feat_tag] += incr\n",
    "                    \n",
    "                    # Compatibility features: explicit marking of follower and followee labels together\n",
    "                    for identity_label_follower in identity_category_follower:\n",
    "                        if identity_label_follower in category_vocabs[identity_category]:\n",
    "                            feat_tag = ('cat=%s,follower_lab=%s,followee_lab=%s' % (identity_category,\n",
    "                                                                                    identity_label_follower,\n",
    "                                                                                    identity_label_followee))\n",
    "                            features[feat_tag] += incr\n",
    "            \n",
    "                \n",
    "    if label == 1:\n",
    "        _extract_features_experiment_2_candidate(nonreblog_candidate, incr=-1)\n",
    "        _extract_features_experiment_2_candidate(reblog_candidate, incr=1)\n",
    "    else:\n",
    "        _extract_features_experiment_2_candidate(reblog_candidate, incr=-1)\n",
    "        _extract_features_experiment_2_candidate(nonreblog_candidate, incr=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  5.5min remaining: 12.9min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.626980229279\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for (reblog_candidate, nonreblog_candidate), label in zip(instances, instance_labels):\n",
    "    X.append(extract_features_post_baseline(reblog_candidate, nonreblog_candidate, label))\n",
    "    y.append(label)\n",
    "    \n",
    "post_features_vectorizer = feature_extraction.DictVectorizer()\n",
    "post_features_scaler = preprocessing.StandardScaler(with_mean=False) # normalization standard scaler\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "X_train = post_features_vectorizer.fit_transform(X_train)\n",
    "X_train = post_features_scaler.fit_transform(X_train)\n",
    "X_test = post_features_vectorizer.transform(X_test)\n",
    "X_test = post_features_scaler.transform(X_test)\n",
    "\n",
    "baseline_model = linear_model.LogisticRegressionCV(cv=10, n_jobs=10, max_iter=1000, verbose=2).fit(X_train, y_train) # default 5 folds\n",
    "print(baseline_model.score(X_test, y_test))\n",
    "baseline_pred = baseline_model.predict(X_test)\n",
    "\n",
    "# Save predictions\n",
    "np.savetxt(os.path.join(data_dirpath, 'results', 'baseline.txt'), baseline_pred)\n",
    "\n",
    "# Save classifier (with weights)\n",
    "with open(os.path.join(data_dirpath, 'models', 'lr_baseline.pkl'), 'wb') as f:\n",
    "    pickle.dump(baseline_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Identity framing, presence of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  4.4min remaining: 10.3min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  9.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.628411466738\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "category = 'zodiac'\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for (reblog_candidate, nonreblog_candidate), label in zip(instances, instance_labels):\n",
    "    X.append(extract_features_experiment_1(reblog_candidate, nonreblog_candidate, label, [category]))\n",
    "    y.append(label)\n",
    "    \n",
    "features_vectorizer_experiment_1 = feature_extraction.DictVectorizer()\n",
    "features_scaler_experiment_1 = preprocessing.StandardScaler(with_mean=False)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "X_train = features_vectorizer_experiment_1.fit_transform(X_train)\n",
    "X_train = features_scaler_experiment_1.fit_transform(X_train)\n",
    "X_test = features_vectorizer_experiment_1.transform(X_test)\n",
    "X_test = features_scaler_experiment_1.transform(X_test)\n",
    "\n",
    "experiment_1_model = linear_model.LogisticRegressionCV(cv=10, n_jobs=10, max_iter=1000, verbose=2).fit(X_train, y_train)\n",
    "print(experiment_1_model.score(X_test, y_test))\n",
    "experiment_1_pred = experiment_1_model.predict(X_test)\n",
    "\n",
    "# Save predictions\n",
    "np.savetxt(os.path.join(data_dirpath, 'results', f'baseline_exp1_{category.replace(\"/\", \"_\").replace(\" \", \"_\")}.txt'), experiment_1_pred)\n",
    "\n",
    "# Save classifier (with weights)\n",
    "with open(os.path.join(data_dirpath, 'models', f'lr_baseline_exp1_{category.replace(\"/\", \"_\").replace(\" \", \"_\")}.pkl'), 'wb') as f:\n",
    "    pickle.dump(experiment_1_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done   7 out of  10 | elapsed: 10.8min remaining:  4.6min\n",
      "[Parallel(n_jobs=5)]: Done  10 out of  10 | elapsed: 11.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.641678476714\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for (reblog_candidate, nonreblog_candidate), label in zip(instances, instance_labels):\n",
    "    X.append(extract_features_experiment_2(reblog_candidate, nonreblog_candidate, label))\n",
    "    y.append(label)\n",
    "    \n",
    "features_vectorizer_experiment_2 = feature_extraction.DictVectorizer()\n",
    "features_scaler_experiment_2 = preprocessing.StandardScaler(with_mean=False)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "X_train = features_vectorizer_experiment_2.fit_transform(X_train)\n",
    "X_train = features_scaler_experiment_2.fit_transform(X_train)\n",
    "X_test = features_vectorizer_experiment_2.transform(X_test)\n",
    "X_test = features_scaler_experiment_2.transform(X_test)\n",
    "\n",
    "experiment_2_model = linear_model.LogisticRegressionCV(cv=10, max_iter=1000, n_jobs=5, verbose=2).fit(X_train, y_train)\n",
    "print(experiment_2_model.score(X_test, y_test))\n",
    "experiment_2_pred = experiment_2_model.predict(X_test)\n",
    "\n",
    "# Save predictions\n",
    "np.savetxt(os.path.join(data_dirpath, 'results', 'baseline_exp1_exp2.txt'), experiment_2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McNemar's Test (Significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88419, 947], [1151, 52017]]\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 0 # Baseline correct, experiment incorrect\n",
    "c = 0 # Baseline incorrect, experiment correct\n",
    "d = 0\n",
    "for b_pred, ex_pred, true in zip(baseline_pred, experiment_1_pred, y_test):\n",
    "    if b_pred == true and ex_pred == true:\n",
    "        a += 1\n",
    "    elif b_pred == true and ex_pred != true:\n",
    "        b += 1\n",
    "    elif b_pred != true and ex_pred == true:\n",
    "        c += 1\n",
    "    else:\n",
    "        d += 1\n",
    "        \n",
    "table = [[a, b],\n",
    "         [c, d]]\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=19.836, p-value=0.000008\n",
      "Different proportions of errors (reject H0)\n"
     ]
    }
   ],
   "source": [
    "# Example of calculating the mcnemar test\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# calculate mcnemar test\n",
    "result = mcnemar(table, exact=False, correction=False)\n",
    "# summarize the finding\n",
    "print('statistic=%.3f, p-value=%.6f' % (result.statistic, result.pvalue))\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if result.pvalue > alpha:\n",
    "\tprint('Same proportions of errors (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different proportions of errors (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load model\n",
    "cat = 'pronouns'\n",
    "model_path = os.path.join(data_dirpath, 'models', 'lr_baseline+exp1_pronouns_filtered.pkl')\n",
    "with open(model_path, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature vectorizer\n",
    "path = os.path.join(data_dirpath, 'results', 'lr_baseline_pronouns_feature_vec.pkl')\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    features_vectorizer_experiment_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "def print_informative_features(feature_vectorizer, model, n=10000):\n",
    "    feats_index2name = {v: k for k, v in features_vectorizer_experiment_1.vocabulary_.items()}\n",
    "    feature_weights = model.coef_[0]\n",
    "    \n",
    "    top_indices = np.argsort(feature_weights)[-1*n:]\n",
    "    top_weights = np.sort(feature_weights)[-1*n:]\n",
    "    bottom_indices = np.argsort(feature_weights)[:n]\n",
    "    bottom_weights = np.sort(feature_weights)[:n]\n",
    "#     tb_weights = sorted(np.hstack([top_weights, bottom_weights], key=lambda row: np.abs(row)))\n",
    "    \n",
    "#     bottom_feats = set(feats_index2name[j] for j in bottom_indices)\n",
    "#     top_feats = set(feats_index2name[j] for j in bottom_indices)\n",
    "\n",
    "    lines = [] # to sort and print\n",
    "    \n",
    "    for i, (j, w) in enumerate(zip(reversed(top_indices), reversed(top_weights))):\n",
    "        feature_name = feats_index2name[j]\n",
    "        if not feature_name.startswith('tag'):\n",
    "            lines.append([i, feature_name, w, abs(w)])\n",
    "#             print(f\"{i}\\t{feature_name}\\t{w: .3f}\")\n",
    "            \n",
    "    for i, (j, w) in enumerate(zip(bottom_indices, bottom_weights)):\n",
    "        feature_name = feats_index2name[j]\n",
    "        if not feature_name.startswith('tag'):\n",
    "            lines.append([i, feature_name, w, abs(w)])\n",
    "#             print(f\"{i}\\t{feature_name}\\t{w: .3f}\")\n",
    "#         print(f\"{i}\\t{feature_name}\\t{w: .3f}\")\n",
    "\n",
    "    for l in list(reversed(sorted(lines, key=itemgetter(3)))):\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 'post_type=answer', -0.9529166535554745, 0.9529166535554745]\n",
      "[26, 'post_type=video', 0.15112336801370957, 0.15112336801370957]\n",
      "[42, 'post_type=photo', 0.14049289929989009, 0.14049289929989009]\n",
      "[491, 'post_type=link', -0.063957875935738193, 0.063957875935738193]\n",
      "[330, 'post_type=text', 0.060734708732589279, 0.060734708732589279]\n",
      "[424, 'post_type=quote', 0.052820641574165633, 0.052820641574165633]\n",
      "[655, 'post_type=chat', 0.039829447750114044, 0.039829447750114044]\n",
      "[1019, 'post_note_count', 0.028999594056302563, 0.028999594056302563]\n",
      "[4051, 'mismatched_follower_presents_cat=pronouns', -0.020885706207246293, 0.020885706207246293]\n",
      "[4050, 'num_mismatched_follower_presents', -0.020885706207246293, 0.020885706207246293]\n",
      "[1486, 'aligned_cat=pronouns', 0.020885706207246293, 0.020885706207246293]\n",
      "[1485, 'num_matches', 0.020885706207246293, 0.020885706207246293]\n",
      "[7236, 'post_type=audio', -0.0093910114863629598, 0.0093910114863629598]\n",
      "[7052, 'post_type=audio', -0.0093910114863629598, 0.0093910114863629598]\n",
      "[7624, 'xor_cat=pronouns', -0.0076057336468349727, 0.0076057336468349727]\n",
      "[6664, 'xor_cat=pronouns', -0.0076057336468349727, 0.0076057336468349727]\n",
      "[4222, 'num_mismatched_followee_presents', 0.00029174813785179924, 0.00029174813785179924]\n",
      "[4221, 'mismatched_followee_presents_cat=pronouns', 0.00029174813785179924, 0.00029174813785179924]\n"
     ]
    }
   ],
   "source": [
    "print_informative_features(features_vectorizer_experiment_1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_features(feature_vectorizer, model, n=100):\n",
    "    feats_index2name = {v: k for k, v in features_vectorizer_experiment_1.vocabulary_.items()}\n",
    "    feature_weights = experiment_1_model.coef_[0]\n",
    "    \n",
    "    top_indices = np.argsort(feature_weights)[-1*n:]\n",
    "    top_weights = np.sort(feature_weights)[-1*n:]\n",
    "    \n",
    "    for i, (j, w) in enumerate(zip(reversed(top_indices), reversed(top_weights))):\n",
    "        feature_name = feats_index2name[j]\n",
    "        if not feature_name.startswith('tag'):\n",
    "            print(f\"{i}\\t{feature_name}\\t{w: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def print_bottom_features(feature_vectorizer, model, n=100):\n",
    "    feats_index2name = {v: k for k, v in features_vectorizer_experiment_1.vocabulary_.items()}\n",
    "    feature_weights = experiment_1_model.coef_[0]\n",
    "    \n",
    "    bottom_indices = np.argsort(feature_weights)[:n]\n",
    "    bottom_weights = np.sort(feature_weights)[:n]\n",
    "#     set_trace()\n",
    "    \n",
    "    for i, (j, w) in enumerate(zip(bottom_indices, bottom_weights)):\n",
    "        feature_name = feats_index2name[j]\n",
    "        if not feature_name.startswith('tag'):\n",
    "            print(f\"{i}\\t{feature_name}\\t{w: .3f}\")\n",
    "#         print(f\"{i}\\t{feature_name}\\t{w: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\tpost_type=video\t 0.143\n",
      "11\tpost_type=photo\t 0.125\n",
      "166\tpost_type=text\t 0.057\n",
      "182\tpost_type=quote\t 0.054\n",
      "213\tmismatched_followee_presents_cat=location\t 0.052\n",
      "335\tpost_type=chat\t 0.043\n",
      "381\taligned_cat=pronouns\t 0.041\n",
      "415\txor_cat=location\t 0.039\n",
      "629\tpost_note_count\t 0.031\n",
      "792\tmismatched_followee_presents_cat=weight\t 0.027\n",
      "824\txor_cat=weight\t 0.026\n"
     ]
    }
   ],
   "source": [
    "print_top_features(features_vectorizer_experiment_1, experiment_1_model, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tpost_type=answer\t-0.884\n",
      "144\tmismatched_followee_presents_cat=interests\t-0.073\n",
      "192\txor_cat=interests\t-0.065\n",
      "226\tmismatched_followee_presents_cat=age\t-0.062\n",
      "245\tpost_type=link\t-0.060\n",
      "347\txor_cat=age\t-0.052\n",
      "664\tmismatched_follower_presents_cat=pronouns\t-0.041\n"
     ]
    }
   ],
   "source": [
    "print_bottom_features(features_vectorizer_experiment_1, experiment_1_model, n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for when categories match, don't match on presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category, what is accuracy when follower and followee align for either reblog or nonreblog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identity_categories = ['age']\n",
    "identity_categories = ['age', 'ethnicity/nationality', 'fandoms', 'gender', 'interests', 'location', 'personality type',\n",
    "                        'pronouns', 'relationship status', 'roleplay', 'sexual orientation', 'zodiac']\n",
    "len(identity_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load themes\n",
    "with open(os.path.join(data_dirpath, 'themes.pkl'), 'rb') as f:\n",
    "    themes = pickle.load(f)\n",
    "\n",
    "print(len(themes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712670\n",
      "712670\n",
      "Index(['post_id', 'tumblog_id_follower', 'tumblog_id_followee', 'post_tags',\n",
      "       'post_type', 'post_note_count', 'processed_blog_description_follower',\n",
      "       'processed_blog_description_followee', 'age_terms_follower',\n",
      "       'age_terms_followee', 'ethnicity/nationality_terms_follower',\n",
      "       'ethnicity/nationality_terms_followee', 'fandoms_terms_follower',\n",
      "       'fandoms_terms_followee', 'gender_terms_follower',\n",
      "       'gender_terms_followee', 'gender/sexuality_terms_follower',\n",
      "       'gender/sexuality_terms_followee', 'interests_terms_follower',\n",
      "       'interests_terms_followee', 'location_terms_follower',\n",
      "       'location_terms_followee', 'personality type_terms_follower',\n",
      "       'personality type_terms_followee', 'pronouns_terms_follower',\n",
      "       'pronouns_terms_followee', 'relationship status_terms_follower',\n",
      "       'relationship status_terms_followee', 'roleplay_terms_follower',\n",
      "       'roleplay_terms_followee', 'roleplay/fandoms_terms_follower',\n",
      "       'roleplay/fandoms_terms_followee', 'sexual orientation_terms_follower',\n",
      "       'sexual orientation_terms_followee', 'weight_terms_follower',\n",
      "       'weight_terms_followee', 'zodiac_terms_follower',\n",
      "       'zodiac_terms_followee'],\n",
      "      dtype='object')\n",
      "(641403, 38)\n",
      "(641403, 38)\n"
     ]
    }
   ],
   "source": [
    "# Load sample1k training set\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import os\n",
    "data_dirpath = '/usr2/mamille2/tumblr/data/sample1k'\n",
    "\n",
    "reblog_feats = pd.read_csv(os.path.join(data_dirpath, 'feature_tables', 'reblog_features.csv'))\n",
    "nonreblog_feats = pd.read_csv(os.path.join(data_dirpath, 'feature_tables', 'nonreblog_features.csv'))\n",
    "print(len(reblog_feats))\n",
    "print(len(nonreblog_feats))\n",
    "print(reblog_feats.columns)\n",
    "\n",
    "# Load instance labels\n",
    "labels_df = pd.read_csv(os.path.join(data_dirpath, 'feature_tables', 'ranking_labels.csv'))\n",
    "labels = labels_df['ranking_label'].values.tolist()[:len(reblog_feats)]\n",
    "\n",
    "train = {}\n",
    "\n",
    "train['reblogs'], _, _, _ = model_selection.train_test_split(reblog_feats, labels, test_size=0.1, random_state=12345)\n",
    "train['nonreblogs'], _, y_train, _ = model_selection.train_test_split(nonreblog_feats, labels, test_size=0.1, random_state=12345)\n",
    "print(train['reblogs'].shape)\n",
    "print(train['nonreblogs'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6669b90ba534474fb83db89e76654add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr0/home/mamille2/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr0/home/mamille2/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr0/home/mamille2/anaconda3/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import itertools\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "overlaps = {c: {'reblogs': [], 'nonreblogs': []} for c in identity_categories}\n",
    "\n",
    "presence_matches = {'reblogs': {c: list() for c in identity_categories},\n",
    "                    'nonreblogs': {c: list() for c in identity_categories}} # cat: set((i, follower_terms, followee_terms), ...)\n",
    "\n",
    "# Load training set predictions, build presence matches, etc\n",
    "for category in tqdm(identity_categories):\n",
    "    model_name = f'lr_baseline+exp1_{category.replace(\"/\", \"_\").replace(\" \", \"_\")}'\n",
    "    pred_fpath = os.path.join(data_dirpath, 'output', 'predictions', f'{model_name}_train_preds.txt')\n",
    "    \n",
    "    preds = np.loadtxt(pred_fpath)\n",
    "\n",
    "    for r in ['reblogs', 'nonreblogs']:\n",
    "        for follower_id, followee_id, follower_terms, followee_terms, pred, actual in zip(\n",
    "                    train[r][f'tumblog_id_follower'], \n",
    "                    train[r][f'tumblog_id_followee'], \n",
    "                    train[r][f'{category}_terms_follower'], \n",
    "                    train[r][f'{category}_terms_followee'],\n",
    "                    preds,\n",
    "                    y_train\n",
    "                ):\n",
    "            \n",
    "            if pred == actual:\n",
    "                pred_str = 'correct_prediction'\n",
    "            else:\n",
    "                pred_str = 'incorrect_prediction'\n",
    "            \n",
    "            follower_terms = eval(follower_terms)\n",
    "            followee_terms = eval(followee_terms)\n",
    "            if len(follower_terms) > 0 and len(followee_terms) > 0:\n",
    "                follower_themes = []\n",
    "                followee_themes = []\n",
    "                \n",
    "                for f in follower_terms:\n",
    "                    follower_themes += themes[category][f.lower()]\n",
    "                for f in followee_terms:\n",
    "                    followee_themes += themes[category][f.lower()]\n",
    "                \n",
    "                # Specific values of themes (including multiples)\n",
    "#                 presence_matches[r][category].append(\n",
    "#                     (follower_id, followee_id, \n",
    "# #                     set([f.lower() for f in follower_terms]), \n",
    "# #                     set([f.lower() for f in followee_terms]),\n",
    "#                     set(follower_themes), \n",
    "#                     set(followee_themes),\n",
    "#                     pred_str\n",
    "#                     ))\n",
    "                \n",
    "                # No multiples\n",
    "                for follower_theme, followee_theme in itertools.product(follower_themes, followee_themes):\n",
    "                    presence_matches[r][category].append(\n",
    "                        (follower_id, followee_id, \n",
    "                        follower_theme, \n",
    "                        followee_theme,\n",
    "                        pred_str\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'intersection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8884c2a16807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'reblogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nonreblogs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpresence_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0moverlaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverlaps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'intersection'"
     ]
    }
   ],
   "source": [
    "# Measure overlap\n",
    "for category in identity_categories:\n",
    "    for r in ['reblogs', 'nonreblogs']:\n",
    "        for instance in presence_matches[r][category]:\n",
    "            overlaps[category][r].append(len(instance[-3].intersection(instance[-2]))/len(instance[-3].union(instance[-2])))\n",
    "        \n",
    "for category in overlaps:\n",
    "    for r in ['reblogs', 'nonreblogs']:\n",
    "        print(f'{category} {r}: {np.mean(overlaps[category][r])}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy for each category label pairing for either reblog or nonreblog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "['teens, 30+', 294, 0.9285714285714286]\n",
      "['early-20s, early-20s', 36594, 0.70232825053287429]\n",
      "['late-20s, early-20s', 1852, 0.68682505399568039]\n",
      "['30+, 30+', 788, 0.67131979695431476]\n",
      "['early-20s, teens', 22450, 0.6334521158129176]\n",
      "['late-20s, 30+', 221, 0.6244343891402715]\n",
      "['30+, late-20s', 250, 0.60799999999999998]\n",
      "['late-20s, teens', 563, 0.60213143872113672]\n",
      "['teens, late-20s', 333, 0.58858858858858853]\n",
      "['early-20s, 30+', 3519, 0.58141517476555837]\n",
      "['early-20s, late-20s', 5179, 0.54991311063911952]\n",
      "['30+, teens', 1398, 0.54935622317596566]\n",
      "['teens, teens', 2571, 0.53908984830805129]\n",
      "['teens, early-20s', 4909, 0.53147280505194539]\n",
      "['late-20s, late-20s', 206, 0.529126213592233]\n",
      "['30+, early-20s', 1353, 0.46119733924611972]\n",
      "\n",
      "ethnicity/nationality\n",
      "['race/ethnicity, race/ethnicity', 29, 0.96551724137931039]\n",
      "['race/ethnicity, nationality', 387, 0.86046511627906974]\n",
      "['nationality, race/ethnicity', 893, 0.73348264277715569]\n",
      "['nationality, nationality', 419, 0.6467780429594272]\n",
      "\n",
      "fandoms\n",
      "['comics, general fandom', 3037, 0.81791241356601907]\n",
      "['tv/movies, general fandom', 3352, 0.80877088305489264]\n",
      "['specific fandom, general fandom', 4061, 0.7978330460477715]\n",
      "['specific fandom, comics', 2446, 0.78250204415372038]\n",
      "['general fandom, comics', 2876, 0.75556328233657855]\n",
      "['general fandom, specific fandom', 5237, 0.75520336070269234]\n",
      "['specific fandom, game', 3225, 0.73891472868217056]\n",
      "['general fandom, general fandom', 6497, 0.73541634600584882]\n",
      "['general fandom, game', 3105, 0.72367149758454108]\n",
      "['comics, tv/movies', 2980, 0.72214765100671141]\n",
      "['comics, specific fandom', 3846, 0.7204888195527821]\n",
      "['general fandom, tv/movies', 3513, 0.71477369769427834]\n",
      "['tv/movies, specific fandom', 5443, 0.67701635127686932]\n",
      "['tv/movies, tv/movies', 4191, 0.67501789549033642]\n",
      "['specific fandom, specific fandom', 7427, 0.66864144338225395]\n",
      "['specific fandom, tv/movies', 5388, 0.65608760207869343]\n",
      "['game, specific fandom', 3482, 0.60941987363584149]\n",
      "['literature, specific fandom', 3106, 0.59368963296844812]\n",
      "['game, tv/movies', 2332, 0.56861063464837047]\n",
      "['specific fandom, literature', 3124, 0.52560819462227915]\n",
      "\n",
      "gender\n",
      "['trans, family', 309, 0.94498381877022652]\n",
      "['trans, female', 929, 0.90635091496232512]\n",
      "['family, male', 144, 0.77083333333333337]\n",
      "['family, family', 389, 0.74550128534704374]\n",
      "['family, female', 1224, 0.73611111111111116]\n",
      "['female, cis', 309, 0.71521035598705507]\n",
      "['trans, trans', 911, 0.71459934138309555]\n",
      "['female, family', 647, 0.70788253477588869]\n",
      "['trans, male', 668, 0.68562874251497008]\n",
      "['female, female', 2979, 0.64451158106747231]\n",
      "['female, trans', 712, 0.6348314606741573]\n",
      "['male, female', 3688, 0.62337310195227769]\n",
      "['cis, trans', 149, 0.6174496644295302]\n",
      "['female, male', 1238, 0.61550888529886916]\n",
      "['family, cis', 60, 0.59999999999999998]\n",
      "['cis, cis', 22, 0.59090909090909094]\n",
      "['family, trans', 39, 0.5641025641025641]\n",
      "['male, male', 18085, 0.56306331213713023]\n",
      "['male, family', 1481, 0.49426063470627957]\n",
      "['male, trans', 484, 0.35330578512396693]\n",
      "\n",
      "interests\n",
      "['personal care, lifestyle', 1763, 0.81792399319342035]\n",
      "['lifestyle, food/drink', 978, 0.79856850715746419]\n",
      "['art, fitness', 133, 0.79699248120300747]\n",
      "['art, lifestyle', 13027, 0.7894373224840715]\n",
      "['pets, pets', 1207, 0.78376139188069593]\n",
      "['pets, lifestyle', 9595, 0.78249088066701411]\n",
      "['lifestyle, lifestyle', 17557, 0.77775246340490978]\n",
      "['pets, personal care', 1365, 0.76996336996337]\n",
      "['pets, art', 4881, 0.76951444376152422]\n",
      "['art, art', 27378, 0.76890203813280733]\n",
      "['lifestyle, art', 14561, 0.75592335691229995]\n",
      "['pets, food/drink', 336, 0.7321428571428571]\n",
      "['art, pets', 2958, 0.73123732251521301]\n",
      "['lifestyle, pets', 2033, 0.72602065912444658]\n",
      "['lifestyle, personal care', 1887, 0.72496025437201905]\n",
      "['food/drink, art', 389, 0.70437017994858608]\n",
      "['art, food/drink', 3701, 0.69656849500135098]\n",
      "['art, personal care', 1124, 0.68327402135231319]\n",
      "['personal care, art', 9307, 0.60932631352745248]\n",
      "['personal care, personal care', 18920, 0.42108879492600421]\n",
      "\n",
      "location\n",
      "['asia, asia', 2614, 0.73603672532517217]\n",
      "['country, asia', 2735, 0.73601462522851924]\n",
      "['asia, europe', 3396, 0.68168433451118959]\n",
      "['asia, country', 8312, 0.67625120307988451]\n",
      "['regions, americas', 8060, 0.65893300248138953]\n",
      "['regions, regions', 6768, 0.65174349881796689]\n",
      "['country, europe', 4441, 0.65030398558883129]\n",
      "['country, country', 11537, 0.64930224495102717]\n",
      "['americas, cities', 2936, 0.64543596730245234]\n",
      "['regions, country', 3686, 0.63917525773195871]\n",
      "['americas, country', 7349, 0.62539120968839301]\n",
      "['americas, regions', 9728, 0.61533717105263153]\n",
      "['americas, europe', 3506, 0.61409013120365086]\n",
      "['americas, americas', 14318, 0.61251571448526332]\n",
      "['regions, europe', 2414, 0.60729080364540178]\n",
      "['country, americas', 10831, 0.58221770842950793]\n",
      "['country, regions', 6490, 0.57118644067796609]\n",
      "['asia, americas', 7333, 0.56716214373380613]\n",
      "['asia, regions', 5046, 0.54379706698374952]\n",
      "['cities, americas', 3358, 0.49434187016080999]\n",
      "\n",
      "personality type\n",
      "['psychological, dnd', 12, 1.0]\n",
      "['hogwarts, hogwarts', 621, 0.7375201288244766]\n",
      "['hogwarts, psychological', 806, 0.71215880893300243]\n",
      "['psychological, hogwarts', 33, 0.66666666666666663]\n",
      "['psychological, psychological', 129, 0.64341085271317833]\n",
      "\n",
      "pronouns\n",
      "['she/her, neutral', 15146, 0.75419252607949294]\n",
      "['she/her, she/her', 22646, 0.70202243221761018]\n",
      "['she/her, he/his', 6766, 0.67839195979899503]\n",
      "['he/his, he/his', 3559, 0.67743748243888735]\n",
      "['he/his, neutral', 4720, 0.6379237288135593]\n",
      "['he/his, she/her', 6911, 0.63261467226161194]\n",
      "['neutral, neutral', 6565, 0.62848438690022845]\n",
      "['neutral, she/her', 8109, 0.62350474781107412]\n",
      "['neutral, he/his', 7165, 0.60223307745987442]\n",
      "\n",
      "relationship status\n",
      "['single, single', 2, 1.0]\n",
      "['attached, single', 88, 0.85227272727272729]\n",
      "['attached, attached', 746, 0.81769436997319034]\n",
      "['single, attached', 50, 0.47999999999999998]\n",
      "\n",
      "roleplay\n",
      "['general roleplay, general roleplay', 303, 0.6633663366336634]\n",
      "['general roleplay, selective', 21, 0.5714285714285714]\n",
      "\n",
      "sexual orientation\n",
      "['straight, straight', 532, 0.9492481203007519]\n",
      "['lesbian/gay, queer', 554, 0.82851985559566788]\n",
      "['straight, lesbian/gay', 380, 0.80526315789473679]\n",
      "['lesbian/gay, bi/pan', 215, 0.8046511627906977]\n",
      "['lesbian/gay, lesbian/gay', 209, 0.77511961722488043]\n",
      "['bi/pan, asexual', 370, 0.77027027027027029]\n",
      "['queer, asexual', 1092, 0.74358974358974361]\n",
      "['asexual, lesbian/gay', 373, 0.74262734584450407]\n",
      "['asexual, bi/pan', 666, 0.73423423423423428]\n",
      "['bi/pan, queer', 2785, 0.73357271095152599]\n",
      "['asexual, queer', 1976, 0.72722672064777327]\n",
      "['asexual, asexual', 689, 0.72423802612481858]\n",
      "['queer, bi/pan', 2436, 0.7183908045977011]\n",
      "['bi/pan, bi/pan', 1160, 0.7163793103448276]\n",
      "['queer, queer', 7629, 0.71398610564949538]\n",
      "['bi/pan, lesbian/gay', 957, 0.68756530825496343]\n",
      "['queer, lesbian/gay', 3076, 0.6677503250975293]\n",
      "['queer, straight', 264, 0.49621212121212122]\n",
      "['straight, queer', 2823, 0.33581296493092455]\n",
      "['straight, bi/pan', 2415, 0.26045548654244305]\n",
      "\n",
      "zodiac\n",
      "['aries, aries', 2, 1.0]\n",
      "['cancer, aries', 2, 1.0]\n",
      "['libra, libra', 4, 1.0]\n",
      "['aries, virgo', 5, 1.0]\n",
      "['aries, cancer', 5, 1.0]\n",
      "['taurus, cancer', 14, 0.9285714285714286]\n",
      "['taurus, libra', 17, 0.88235294117647056]\n",
      "['taurus, leo', 48, 0.8125]\n",
      "['aries, taurus', 4, 0.75]\n",
      "['cancer, taurus', 4, 0.75]\n",
      "['taurus, aries', 76, 0.75]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "# overlap_counter = {c: {'reblogs': Counter(), 'nonreblogs': Counter()} for c in identity_categories}\n",
    "overlap_counter = {c: Counter() for c in identity_categories}\n",
    "correct_preds = {c: defaultdict(list) for c in identity_categories}\n",
    "\n",
    "# Measure overlap\n",
    "for category in identity_categories:\n",
    "    for r in ['reblogs', 'nonreblogs']:\n",
    "        for instance in presence_matches[r][category]:\n",
    "#             value_pairing = (tuple(instance[-3]), tuple(instance[-2]))\n",
    "            value_pairing = (instance[-3], instance[-2])\n",
    "            overlap_counter[category][value_pairing] += 1\n",
    "            if instance[-1].startswith('correct'):\n",
    "                correct_preds[category][value_pairing].append(1)\n",
    "            else:\n",
    "                correct_preds[category][value_pairing].append(0)\n",
    "        \n",
    "for category in overlap_counter:\n",
    "    lines = []\n",
    "    \n",
    "    print(category)\n",
    "    for (labels1, labels2), count in overlap_counter[category].most_common(20):\n",
    "        lines.append([f'{labels1}, {labels2}', count, np.mean(correct_preds[category][(labels1, labels2)])])\n",
    "#         print(f'{labels1}, {labels2}\\t{count}\\t{np.mean(correct_preds[category][(labels1, labels2)])}')\n",
    "    for l in list(reversed(sorted(lines, key=itemgetter(-1)))):\n",
    "        print(l)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix (N/A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 43065\n",
      "True negatives: 48379\n",
      "False positives: 22776\n",
      "False negatives: 28314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[48379, 22776], [28314, 43065]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, experiment_1_pred).ravel()\n",
    "print(f\"True positives: {tp}\")\n",
    "print(f\"True negatives: {tn}\")\n",
    "print(f\"False positives: {fp}\")\n",
    "print(f\"False negatives: {fn}\")\n",
    "[[tn, fp],\n",
    "[fn, tp]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific errors\n",
    "Look for specific times when have a feature but is still giving an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = 'aligned_cat=pronouns'\n",
    "feature_index = features_vectorizer_experiment_1.vocab_[feature_name]\n",
    "positive_examples = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

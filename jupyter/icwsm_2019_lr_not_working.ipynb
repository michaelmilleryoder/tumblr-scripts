{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "data_dirpath = '/usr2/mamille2/tumblr/data/sample1k'\n",
    "\n",
    "feature_tables_dir = os.path.join(data_dirpath, 'feature_tables')\n",
    "filenames = ['reblog_features.csv', 'nonreblog_features.csv', 'ranking_labels.csv']\n",
    "joined_filenames = [os.path.join(feature_tables_dir, filename) for filename in filenames]\n",
    "# csv_readers = [csv.DictReader(codecs.open(filename, 'rU', 'utf-16')) for filename in joined_filenames]\n",
    "csv_readers = [csv.DictReader(x.replace('\\0', '') for x in open(filename, 'r')) for filename in joined_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712670 712670\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "instance_labels = []\n",
    "for row in zip(*csv_readers):\n",
    "    reblog_features = row[0]\n",
    "    nonreblog_features = row[1]\n",
    "    label = int(row[2]['ranking_label'])\n",
    "    instance = (reblog_features, nonreblog_features) # reblog always first, nonreblog always second\n",
    "    instances.append(instance)\n",
    "    instance_labels.append(label)\n",
    "    \n",
    "print(len(instances), len(instance_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tag vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14318\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def _str2list(in_str):\n",
    "    return [el[1:-1] for el in in_str[1:-1].split(', ')]\n",
    "\n",
    "def update_tag_counts(tag_counts, counted_ids, candidate): # for hashtags\n",
    "#     candidate_tags = [tag.lower() for tag in eval(candidate['post_tags'])] # uses tokens provided in feature tables\n",
    "    candidate_tags = [tag.lower() for tag in _str2list(candidate['post_tags'])] # uses tokens provided in feature tables\n",
    "    followee_id = candidate['tumblog_id_followee']    \n",
    "    for tag in candidate_tags:\n",
    "        if not followee_id in counted_ids[tag]: # only counts the tag if user hasn't already used the tag\n",
    "            tag_counts[tag] += 1\n",
    "            counted_ids[tag].add(followee_id)\n",
    "        \n",
    "counted_ids = defaultdict(lambda: set()) # for each tag, a set of followees who used those tags\n",
    "tag_counts = defaultdict(int) # count of unique followees who used each tag\n",
    "for reblog_candidate, nonreblog_candidate in instances:\n",
    "    update_tag_counts(tag_counts, counted_ids, reblog_candidate)\n",
    "    update_tag_counts(tag_counts, counted_ids, nonreblog_candidate)\n",
    "\n",
    "tag_counts_filtered = {k:v for k,v in tag_counts.items() if v > 1} # at least 2 users used the tag\n",
    "tag_vocab = tag_counts_filtered.keys()\n",
    "print(len(tag_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_categories = ['age', 'ethnicity/nationality', 'fandoms', 'gender',\n",
    "                       'interests', 'location', 'personality type', 'pronouns', 'relationship status', 'roleplay',\n",
    "                       'sexual orientation', 'weight', 'zodiac']\n",
    "len(identity_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count category label instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_label_counts = defaultdict(lambda: defaultdict(int)) # {category: {value: count_of_unique_users}}\n",
    "# counted_ids = set()\n",
    "for category in identity_categories:\n",
    "    counted_ids = set() # for each category, ids already considered\n",
    "    for reblog_candidate, nonreblog_candidate in instances:\n",
    "        category_followee = category + '_terms_followee'\n",
    "        followee_id = reblog_candidate['tumblog_id_followee']\n",
    "        if not followee_id in counted_ids: # only counts labels from first instance seen of a followee, since constant\n",
    "            category_value = [x.lower() for x in eval(reblog_candidate[category_followee])]\n",
    "            for value in category_value:\n",
    "                category_label_counts[category][value] += 1\n",
    "            counted_ids.add(followee_id)\n",
    "            \n",
    "        followee_id = nonreblog_candidate['tumblog_id_followee']\n",
    "        if not followee_id in counted_ids:\n",
    "            category_value = [x.lower() for x in eval(nonreblog_candidate[category_followee])]\n",
    "            for value in category_value:\n",
    "                category_label_counts[category][value] += 1\n",
    "            counted_ids.add(followee_id)\n",
    "        \n",
    "        category_follower = category + '_terms_follower'\n",
    "        follower_id = reblog_candidate['tumblog_id_follower']\n",
    "        if not follower_id in counted_ids:\n",
    "            category_value = [x.lower() for x in eval(reblog_candidate[category_follower])]\n",
    "            for value in category_value:\n",
    "                category_label_counts[category][value] += 1\n",
    "            counted_ids.add(follower_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create category label vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age 65\n",
      "{'28', '45', '17', 'fourteen', '27', 'fifteen', '21', '43', '37', '46', '36', '13', 'eighteen', 'nineteen', '53', '20', '35', '19', '10', '33', 'xxiv', '47', '18', 'sixteen', 'y/o', '40', '34', 'twenty', '44', 'seventeen', '11', '26', '24', 'xix', '31', '49', '38', '15', '30', '56', '51', 'xxix', '41', '14', '29', '58', '39', '42', '23', '25', '22', '12', '16', 'forty', 'twelve', 'age', '55', '48', '54', '57', '52', '59', 'fifty', '50', '32'}\n",
      "-----------------\n",
      "\n",
      "ethnicity/nationality 81\n",
      "{'chilean', 'serbian', 'swiss', 'colombian', 'mexican', 'scandinavian', 'danish', 'tigre', 'czech', 'dutch', 'russian', 'hispanic', 'puerto', 'canadian', 'brazilian', 'haitian', 'papel', 'chilena', 'american', 'americans', 'armenian', 'turkish', 'english', 'romanian', 'dakota', 'polish', 'indian', 'african', 'omaha', 'thai', 'venezuelan', 'filipina', 'pakistani', 'coeur', 'finnish', 'albanian', 'white', 'jamaican', 'latino', 'saudi', 'dominican', 'malay', 'swede', 'latina', 'portuguese', 'italian', 'scottish', 'chinese', 'korean', 'japanese', 'turks', 'swedish', 'european', 'chileno', 'ottawa', 'greek', 'malaysian', 'vietnamese', 'spanish', 'southern', 'moroccan', 'hungarian', 'german', 'belgian', 'mexicana', 'singaporean', 'black', 'asian', 'filipino', 'french', 'south african', 'austrian', 'indonesian', 'norwegian', 'irish', 'lithuanian', 'mexicano', 'cuban', 'british', 'thais', 'australian'}\n",
      "-----------------\n",
      "\n",
      "fandoms 54\n",
      "{'voltron', 'multiverse', 'undertale', 'ships', 'twd', 'i ship', 'fan', 'multi-ship', 'rey', 'tjlc', 'shaladin', 'yuri', 'stan', 'shipper', 'comic', 'verse', 'overwatch', 'phan', 'hamilton', 'exo', 'riverdale', 'sherlock', 'marvel', 'army', 'sims', 'multi-fandom', 'ereri', 'fanatic', 'bnha', 'k-pop', 'yaoi', 'series', 'universe', 'mcu', 'disney', 'otaku', 'pokemon', 'kpop', 'harry potter', 'got7', 'wwe', 'multiship', 'potterhead', 'fandom', 'homestuck', 'bts', 'reylo', 'canon', 'hp', '5sos', 'hogwarts', 'star wars', 'swiftie', 'supercorp'}\n",
      "-----------------\n",
      "\n",
      "gender 33\n",
      "{'mtf', 'dad', 'girl', 'nonbinary', 'bigender', 'cis', 'son', 'princess', 'mommy', 'boy', 'husband', 'woman', 'trans', 'nb', 'female', 'daughter', 'ftm', 'wife', 'gurl', 'male', 'lady', 'non-binary', 'mum', 'mom', 'guy', 'queen', 'agender', 'lgbt', 'genderfluid', 'androgynous', 'man', 'sister', 'brother'}\n",
      "-----------------\n",
      "\n",
      "interests 71\n",
      "{'piano', 'arts', 'design', 'interests', 'nutrition', 'tattoos', 'film', 'write', 'comics', 'family', 'animals', 'anime', 'dog', 'dogs', 'reading', 'history', 'movies', 'fitness', 'things', 'memes', 'running', 'pies', 'games', 'hair', 'meme', 'theatre', 'music', 'makeup', 'meche', 'cosplay', 'gifs', 'bands', 'fashion', 'nature', 'clarinet', 'science', 'photos', 'pizza', 'cat', 'tea', 'soccer', 'travel', 'weed', 'photography', 'stuff', 'lifestyle', 'law', 'book', 'food', 'poetry', 'books', 'drawing', 'landscapes', 'writing', 'cats', 'tv', 'desserts', 'horror', 'gaming', 'aesthetics', 'metal', 'manga', 'aesthetic', 'animal', 'robot', 'coffee', 'art', 'draw', 'hockey', 'hunting', 'psychology'}\n",
      "-----------------\n",
      "\n",
      "location 146\n",
      "{'north', 'colorado', 'slovenia', 'm√©xico', 'venezuela', 'atlanta', 'uk', 'arizona', 'north carolina', 'indiana', 'egypt', 'mississippi', 'malaysia', 'philly', 'rhode island', 'hungary', 'bulgaria', 'japan', 'massachusetts', 'alaska', 'chad', 'costa rica', 'thailand', 'jamaica', 'norway', 'philippines', 'south', 'kentucky', 'pennsylvania', 'portugal', 'louisiana', 'toronto', 'utah', 'az', 'pnw', 'athens', 'new zealand', 'ecuador', 'midwest', 'chile', 'arkansas', 'italy', 'serbia', 'czech republic', 'taiwan', 'hawaii', 'florida', 'illinois', 'alabama', 'finland', 'nz', 'poland', 'cali', 'ca', 'albania', 'london', 'dc', 'brazil', 'nc', 'philadelphia', 'south carolina', 'al', 'socal', 'maryland', 'iowa', 'canada', 'ct', 'maine', 'sweden', 'switzerland', 'ga', 'wisconsin', 'chicago', 'pa', 'new york', 'montana', 'austria', 'east', 'atl', 'georgia', 'san', 'new jersey', 'minnesota', 'fl', 'virginia', 'mexico', 'texas', 'belgium', 'iceland', 'greece', 'mali', 'hong kong', 'iÃástanbul', 'berlin', 'germany', 'wa', 'guinea', 'turkey', 'ohio', 'israel', 'u.k', 'denmark', 'new hampshire', 'colombia', 'australia', 'oregon', 'west', 'united states', 'singapore', 'indonesia', 'delaware', 'madagascar', 'vietnam', 'slovakia', 'spain', 'nyc', 'aus', 'istanbul', 'croatia', 'kansas', 'jordan', 'ny', 'india', 'washington', 'tx', 'oklahoma', 'nj', 'mx', 'u.s', 'france', 'california', 'netherlands', 'argentina', 'cleveland', 'umass', 'u.s.', 'south africa', 'romania', 'michigan', 'missouri', 'china', 'va', 'tampa', 'usa', 'ireland', 'tennessee'}\n",
      "-----------------\n",
      "\n",
      "personality type 29\n",
      "{'intp', 'introvert', 'infp', 'extrovert', 'enfj', 'infj', '5w4', 'gryffindor', 'ambivert', '5w6', 'isfp', 'istp', 'lawful', 'enfp', 'ravenclaw', 'entp', 'esfp', 'estp', 'chaotic', 'intj', 'antp', 'entj', 'isfj', '4w5', 'esfj', 'istj', 'neutral', 'slytherin', 'hufflepuff'}\n",
      "-----------------\n",
      "\n",
      "pronouns 9\n",
      "{'xe', 'them', 'pronouns', 'him', 'they', 'her', 'she', 'he', 'itits'}\n",
      "-----------------\n",
      "\n",
      "relationship status 9\n",
      "{'taken', 'couple', 'engaged', 'wife', 'in a relationship', 'husband', 'single', 'married', 'spouse'}\n",
      "-----------------\n",
      "\n",
      "roleplay 7\n",
      "{'muse', 'rp', 'semi-selective', 'selective', 'roleplay', 'm!a', 'oc'}\n",
      "-----------------\n",
      "\n",
      "sexual orientation 18\n",
      "{'bisexual', 'lgbt', 'aro/ace', 'demisexual', 'pan', 'ace', 'wlw', 'bi', 'queer', 'homo', 'pansexual', 'lesbian', 'aro-ace', 'cishet', 'gay', 'mlm', 'straight', 'asexual'}\n",
      "-----------------\n",
      "\n",
      "weight 21\n",
      "{'gw', 'gw2', 'eating disorders', 'kg', 'thin', 'hw', 'pounds', 'anorexic', 'lb', 'ana', 'eating disorder', 'sw', 'gw1', 'lbs', 'cw', 'fat', 'bulimia', 'anorexia', 'gw3', 'weight', 'lw'}\n",
      "-----------------\n",
      "\n",
      "zodiac 7\n",
      "{'aries', 'cancer', 'leo', 'virgo', 'gemini', 'libra', 'taurus'}\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_vocabs = defaultdict(lambda: set())\n",
    "for identity_category in category_label_counts:\n",
    "    category_labels_filtered_vocab = set([k for k,v in category_label_counts[identity_category].items() if v > 1]) # min 2 users using label\n",
    "    category_vocabs[identity_category] = category_labels_filtered_vocab\n",
    "    print(identity_category, len(category_vocabs[identity_category]))\n",
    "    print(category_vocabs[identity_category])\n",
    "    print('-----------------')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    sorted_category_labels = sorted(category_labels_filtered[category].items(), key=lambda x: x[1], reverse=True)    \n",
    "    print(category)\n",
    "    print('-----------------')\n",
    "    for i in range(1, 21):\n",
    "        if i == len(sorted_category_labels):\n",
    "            break\n",
    "        print(sorted_category_labels[i][0], sorted_category_labels[i][1])\n",
    "    print('-----------------')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_post_baseline(reblog_candidate, nonreblog_candidate, label):\n",
    "    features = defaultdict(float) # {feat: count} for each instance\n",
    "    # Comparison space features\n",
    "    def _extract_features_post_baseline_candidate(candidate, incr):\n",
    "        candidate_tags = [tag.lower() for tag in eval(candidate['post_tags'])]\n",
    "        for tag in candidate_tags:\n",
    "            if tag.lower() in tag_vocab:\n",
    "                feat_tag = ('tag=%s' % tag.lower())\n",
    "                features[feat_tag] += incr\n",
    "\n",
    "        post_type = candidate['post_type']\n",
    "        feat_tag = ('post_type=%s' % post_type)\n",
    "        features[feat_tag] += incr\n",
    "        \n",
    "        try:\n",
    "            post_note_count = float(candidate['post_note_count'])\n",
    "        except ValueError as e:\n",
    "            post_note_count = 0.0\n",
    "            \n",
    "        features['post_note_count'] += incr * post_note_count\n",
    "        \n",
    "    # if randomly-generated label is 1, second candidate is reblog, so flip: -1 is whatever candidate should consider first\n",
    "    if label == 1: \n",
    "        _extract_features_post_baseline_candidate(nonreblog_candidate, incr=-1)\n",
    "        _extract_features_post_baseline_candidate(reblog_candidate, incr=1)\n",
    "    else:\n",
    "        _extract_features_post_baseline_candidate(reblog_candidate, incr=-1)\n",
    "        _extract_features_post_baseline_candidate(nonreblog_candidate, incr=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Identity framing, presence of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_experiment_1(reblog_candidate, nonreblog_candidate, label):\n",
    "    # Baseline features\n",
    "    features = defaultdict(float)\n",
    "#     features = extract_features_post_baseline(reblog_candidate, nonreblog_candidate, label)\n",
    "    \n",
    "    # Follower features\n",
    "    for identity_category in identity_categories:\n",
    "        identity_category_follower = eval(reblog_candidate[identity_category + '_terms_follower'])\n",
    "        follower_presence = len(identity_category_follower) > 0\n",
    "        if follower_presence:\n",
    "            feat_tag = ('follower_cat=%s' % identity_category)\n",
    "            features[feat_tag] += 1\n",
    "            \n",
    "    # Comparison space features\n",
    "    def _extract_features_experiment_1_candidate(candidate, incr):\n",
    "        for identity_category in identity_categories:\n",
    "            identity_category_follower = eval(reblog_candidate[identity_category + '_terms_follower'])\n",
    "            follower_presence = len(identity_category_follower) > 0\n",
    "            identity_category_followee = eval(candidate[identity_category + '_terms_followee'])\n",
    "            followee_presence = len(identity_category_followee) > 0\n",
    "            if followee_presence:\n",
    "                feat_tag = ('followee_cat=%s' % identity_category)\n",
    "                features[feat_tag] += incr\n",
    "\n",
    "            # Alignment features\n",
    "            if ((follower_presence and followee_presence) or\n",
    "                (not follower_presence and not followee_presence)):\n",
    "                feat_tag = ('aligned_cat=%s' % identity_category)\n",
    "                features[feat_tag] += incr\n",
    "                \n",
    "    if label == 1:\n",
    "        _extract_features_experiment_1_candidate(nonreblog_candidate, incr=-1)\n",
    "        _extract_features_experiment_1_candidate(reblog_candidate, incr=1)\n",
    "    else:\n",
    "        _extract_features_experiment_1_candidate(reblog_candidate, incr=-1)\n",
    "        _extract_features_experiment_1_candidate(nonreblog_candidate, incr=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_experiment_2(reblog_candidate, nonreblog_candidate, label):\n",
    "    # Baseline features\n",
    "    features = defaultdict(float)\n",
    "    #features = extract_features_post_baseline(reblog_candidate, nonreblog_candidate, label)\n",
    "#     features = extract_features_experiment_1(reblog_candidate, nonreblog_candidate, label)\n",
    "\n",
    "    \n",
    "    # Follower features\n",
    "    for identity_category in identity_categories:\n",
    "        identity_category_follower = [x.lower() for x in eval(reblog_candidate[identity_category + '_terms_follower'])]\n",
    "        for identity_label in identity_category_follower:\n",
    "            if identity_label in category_vocabs[identity_category]:\n",
    "                feat_tag = ('cat=%s,follower_lab=%s' % (identity_category, identity_label))\n",
    "                features[feat_tag] += 1\n",
    "            \n",
    "    # Comparison space features\n",
    "    def _extract_features_experiment_2_candidate(candidate, incr):\n",
    "        for identity_category in identity_categories:\n",
    "            identity_category_follower = [x.lower() for x in eval(reblog_candidate[identity_category + '_terms_follower'])]\n",
    "            identity_category_followee = [x.lower() for x in eval(reblog_candidate[identity_category + '_terms_followee'])]\n",
    "            for identity_label_followee in identity_category_followee:\n",
    "                if identity_label_followee in category_vocabs[identity_category]:\n",
    "                    feat_tag = ('cat=%s,followee_lab=%s' % (identity_category, identity_label_followee))\n",
    "                    features[feat_tag] += incr\n",
    "                    \n",
    "                    # Compatibility features: explicit marking of follower and followee labels together\n",
    "                    for identity_label_follower in identity_category_follower:\n",
    "                        if identity_label_follower in category_vocabs[identity_category]:\n",
    "                            feat_tag = ('cat=%s,follower_lab=%s,followee_lab=%s' % (identity_category,\n",
    "                                                                                    identity_label_follower,\n",
    "                                                                                    identity_label_followee))\n",
    "                            features[feat_tag] += incr\n",
    "            \n",
    "                \n",
    "    if label == 1:\n",
    "        _extract_features_experiment_2_candidate(nonreblog_candidate, incr=-1)\n",
    "        _extract_features_experiment_2_candidate(reblog_candidate, incr=1)\n",
    "    else:\n",
    "        _extract_features_experiment_2_candidate(reblog_candidate, incr=-1)\n",
    "        _extract_features_experiment_2_candidate(nonreblog_candidate, incr=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.627029340368\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for (reblog_candidate, nonreblog_candidate), label in zip(instances, instance_labels):\n",
    "    X.append(extract_features_post_baseline(reblog_candidate, nonreblog_candidate, label))\n",
    "    y.append(label)\n",
    "    \n",
    "post_features_vectorizer = feature_extraction.DictVectorizer()\n",
    "post_features_scaler = preprocessing.StandardScaler(with_mean=False) # normalization standard scaler\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "X_train = post_features_vectorizer.fit_transform(X_train)\n",
    "X_train = post_features_scaler.fit_transform(X_train)\n",
    "X_test = post_features_vectorizer.transform(X_test)\n",
    "X_test = post_features_scaler.transform(X_test)\n",
    "\n",
    "baseline_model = linear_model.LogisticRegressionCV(cv=10, verbose=2).fit(X_train, y_train)\n",
    "print(baseline_model.score(X_test, y_test))\n",
    "baseline_pred = baseline_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Identity framing, presence of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.542923092034\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for (reblog_candidate, nonreblog_candidate), label in zip(instances, instance_labels):\n",
    "    X.append(extract_features_experiment_1(reblog_candidate, nonreblog_candidate, label))\n",
    "    y.append(label)\n",
    "    \n",
    "features_vectorizer_experiment_1 = feature_extraction.DictVectorizer()\n",
    "features_scaler_experiment_1 = preprocessing.StandardScaler(with_mean=False)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "X_train = features_vectorizer_experiment_1.fit_transform(X_train)\n",
    "X_train = features_scaler_experiment_1.fit_transform(X_train)\n",
    "X_test = features_vectorizer_experiment_1.transform(X_test)\n",
    "X_test = features_scaler_experiment_1.transform(X_test)\n",
    "\n",
    "experiment_1_model = linear_model.LogisticRegressionCV(cv=10, verbose=2).fit(X_train, y_train)\n",
    "print(experiment_1_model.score(X_test, y_test))\n",
    "experiment_1_pred = experiment_1_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for (reblog_candidate, nonreblog_candidate), label in zip(instances, instance_labels):\n",
    "    X.append(extract_features_experiment_2(reblog_candidate, nonreblog_candidate, label))\n",
    "    y.append(label)\n",
    "    \n",
    "features_vectorizer_experiment_2 = feature_extraction.DictVectorizer()\n",
    "features_scaler_experiment_2 = preprocessing.StandardScaler(with_mean=False)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "X_train = features_vectorizer_experiment_2.fit_transform(X_train)\n",
    "X_train = features_scaler_experiment_2.fit_transform(X_train)\n",
    "X_test = features_vectorizer_experiment_2.transform(X_test)\n",
    "X_test = features_scaler_experiment_2.transform(X_test)\n",
    "\n",
    "experiment_2_model = linear_model.LogisticRegressionCV(cv=10, verbose=2).fit(X_train, y_train)\n",
    "print(experiment_2_model.score(X_test, y_test))\n",
    "experiment_2_pred = experiment_2_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McNemar's Test (Significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0 # Baseline correct, experiment incorrect\n",
    "c = 0 # Baseline incorrect, experiment correct\n",
    "d = 0\n",
    "for b_pred, ex_pred, true in zip(baseline_pred, experiment_1_pred, y_test):\n",
    "    if b_pred == true and ex_pred == true:\n",
    "        a += 1\n",
    "    elif b_pred == true and ex_pred != true:\n",
    "        b += 1\n",
    "    elif b_pred != true and ex_pred == true:\n",
    "        c += 1\n",
    "    else:\n",
    "        d += 1\n",
    "        \n",
    "table = [[a, b],\n",
    "         [c, d]]\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of calculating the mcnemar test\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# calculate mcnemar test\n",
    "result = mcnemar(table, exact=False, correction=False)\n",
    "# summarize the finding\n",
    "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if result.pvalue > alpha:\n",
    "\tprint('Same proportions of errors (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different proportions of errors (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
